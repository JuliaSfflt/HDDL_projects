import os
import pandas as pd














# Définir le chemin du dossier principal contenant les données
base_dir = 'IMDB_dataset'  # Remplacez par le chemin réel de votre dossier

# Initialiser les DataFrames vides pour les ensembles d'entraînement, de test et non étiquetés
train_df = pd.DataFrame(columns=["text", "label"])
test_df = pd.DataFrame(columns=["text", "label"])
unlabeled_data = pd.DataFrame(columns=["text"])

# Parcourir les dossiers train et test pour charger les fichiers étiquetés
for folder in ['train', 'test']:
    for sentiment in ['pos', 'neg']:
        # Créer le chemin complet pour chaque dossier (pos/neg)
        sentiment_folder = os.path.join(base_dir, folder, sentiment)
        
        # Initialiser une liste pour stocker les données de chaque dossier
        texts = []
        labels = []
        
        # Parcourir tous les fichiers .txt dans le dossier
        for filename in os.listdir(sentiment_folder):
            if filename.endswith('.txt'):
                # Créer le chemin complet du fichier
                file_path = os.path.join(sentiment_folder, filename)
                
                # Ouvrir le fichier et lire son contenu
                with open(file_path, 'r', encoding='utf-8') as file:
                    text = file.read()  # Lire tout le contenu du fichier
                
                # Ajouter le texte et l'étiquette correspondante à la liste
                texts.append(text)
                labels.append(1 if sentiment == 'pos' else 0)  # 1 pour positif, 0 pour négatif
        
        # Créer un DataFrame pour le dossier actuel
        temp_df = pd.DataFrame({"text": texts, "label": labels})
        
        # Ajouter les données au DataFrame d'entraînement ou de test
        if folder == 'train':
            train_df = pd.concat([train_df, temp_df], ignore_index=True)
        elif folder == 'test':
            test_df = pd.concat([test_df, temp_df], ignore_index=True)

# Traiter les données non étiquetées dans le dossier 'train/unsup'
unsup_folder = os.path.join(base_dir, 'train', 'unsup')

# Initialiser une liste pour les textes non étiquetés
unlabeled_texts = []

# Parcourir tous les fichiers .txt dans le dossier unsup
for filename in os.listdir(unsup_folder):
    if filename.endswith('.txt'):
        # Créer le chemin complet du fichier
        file_path = os.path.join(unsup_folder, filename)
        
        # Ouvrir le fichier et lire son contenu
        with open(file_path, 'r', encoding='utf-8') as file:
            text = file.read()  # Lire tout le contenu du fichier
        
        # Ajouter le texte à la liste des données non étiquetées
        unlabeled_texts.append(text)

# Créer un DataFrame pour les données non étiquetées
unlabeled_data = pd.DataFrame({"text": unlabeled_texts})





import matplotlib.pyplot as plt
import seaborn as sns


# Vérifier la taille des DataFrames et afficher le nombre de "pos" et "neg"
print(f"Nombre d'exemples dans le train set : {train_df.shape[0]}")  # Nombre d'exemples d'entraînement
print(f"Nombre d'exemples dans le test set : {test_df.shape[0]}")    # Nombre d'exemples de test
print(f"Nombre d'exemples non étiquetés dans le train/unsup : {unlabeled_data.shape[0]}")  # Nombre de données non étiquetées


# Afficher le nombre de 'pos' et 'neg' dans les train et test sets

plt.figure(figsize=(15,5))
plt.subplot(121)
sns.countplot(train_df["label"])# Affiche la répartition des labels dans le train set
plt.title("La répartition des labels dans le train set")

plt.subplot(122)
sns.countplot(test_df["label"])# Affiche la répartition des labels dans le train set
plt.title("La répartition des labels dans le test set")
plt.show()




# Afficher les premières lignes des DataFrames pour vérification
print("\nExemples du train set :")
print(train_df.head())

print("\nExemples du test set :")
print(test_df.head())

print("\nExemples des données non étiquetées :")
print(unlabeled_data.head())


# Vérification des valeurs nulles
print("\nVérification des valeurs nulles dans le train set :")
print(train_df.isnull().sum())  # Compte les valeurs nulles dans chaque colonne

print("\nVérification des valeurs nulles dans le test set :")
print(test_df.isnull().sum())  # Compte les valeurs nulles dans chaque colonne

print("\nVérification des valeurs nulles dans les données non étiquetées :")
print(unlabeled_data.isnull().sum())  # Compte les valeurs nulles dans chaque colonne


# Vérification des doublons
print("\nVérification des doublons dans le train set :")
print(train_df.duplicated().sum())  # Nombre de lignes dupliquées dans le train set

print("\nVérification des doublons dans le test set :")
print(test_df.duplicated().sum())  # Nombre de lignes dupliquées dans le test set

print("\nVérification des doublons dans les données non étiquetées :")
print(unlabeled_data.duplicated().sum())  # Nombre de lignes dupliquées dans les données non étiquetées

# Extraire les textes de l'ensemble train et test
train_texts = train_df["text"]
test_texts = test_df["text"]

# Trouver l'intersection entre les deux ensembles de textes (train et test)
common_texts = set(train_texts).intersection(set(test_texts))

# Afficher le nombre d'éléments communs (intersection)
print(f"\nNombre de commentaires communs entre l'ensemble train et l'ensemble test : {len(common_texts)}")


# Supprimer les doublons dans train et test
train_df = train_df.drop_duplicates(subset=["text"])
test_df = test_df.drop_duplicates(subset=["text"])
unlabeled_data = unlabeled_data.drop_duplicates(subset=["text"])

# Trouver l'intersection
common_texts = set(train_df["text"]).intersection(set(test_df["text"]))

# Supprimer les commentaires communs dans les deux ensembles
test_df = test_df[~test_df["text"].isin(common_texts)]

# Afficher les tailles après nettoyage
print(f"Train set size after cleaning: {train_df.shape[0]}")
print(f"Test set size after cleaning: {test_df.shape[0]}")


train_df['label'] = train_df['label'].astype('category')
test_df['label'] = test_df['label'].astype('category')


import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.manifold import TSNE
import seaborn as sns

# 1. Appliquer TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)  # Limit to 1000 features
train_tfidf = vectorizer.fit_transform(train_df["text"])

# 2. Appliquer t-SNE pour réduction dimensionnelle à 2D
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(train_tfidf.toarray())



import seaborn as sns
# 3. Tracer le graphique
plt.figure(figsize=(10, 8))
sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=train_df['label'] )
plt.title("t-SNE visualization of the train set")
plt.xlabel("Dimension 1")
plt.ylabel("Dimension 2")
plt.show()


from sklearn.linear_model import LogisticRegression

model = LogisticRegression(max_iter=1000)
model.fit(train_tfidf, train_df['label'])

train_score = model.score(train_tfidf,train_df['label'] )
print(train_score)





















